{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82b1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c3721",
   "metadata": {},
   "source": [
    "# Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519fcd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1518e57b290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_bw = Image.open(r\"C:\\Coding\\Emotion recognition\\Data\\Angry\\10.png\")\n",
    "image_color = Image.open(r\"C:\\Coding\\Emotion recognition\\Data\\Image.jpg\")\n",
    "plt.imshow(image_bw)\n",
    "#displays pixel values as colors\n",
    "\n",
    "# plt.axis(\"off\")\n",
    "#removes coordinate cluste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27d9f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 48]) torch.float32\n",
      "torch.Size([3, 408, 612]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "image_bw_tensor = to_tensor(image_bw)\n",
    "image_color_tensor = to_tensor(image_color)\n",
    "\n",
    "print(image_bw_tensor.shape, image_bw_tensor.dtype)\n",
    "print(image_color_tensor.shape, image_color_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f667b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 59099\n",
      "Classes : ['Angry', 'Fear', 'Happy', 'Sad', 'Suprise']\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "DATA_DIR = r\"C:\\Coding\\Emotion recognition\\Data\"\n",
    "#Load dataset from directory\n",
    "dataset = ImageFolder(root=DATA_DIR)\n",
    "#ImageFolder assumes that images are organized in subdirectories named after their classes\n",
    "# Imagefolder looks at folder names, assigns each folder an interger label, and maps images to those labels based on their folder\n",
    "\"\"\"\n",
    "e.g.\n",
    "Happy : 0\n",
    "Sad : 1\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Number of samples : {len(dataset)}\")\n",
    "print(f\"Classes : {dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270aec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'PIL.Image.Image'>\n",
      "Size (W, H): (48, 48)\n",
      "Mode : RGB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(47.5), np.float64(47.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = dataset[10]\n",
    "\n",
    "print(\"Type: \", type(img))\n",
    "print(f\"Size (W, H): {img.size}\")\n",
    "print(f\"Mode : {img.mode}\")\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Label : {dataset.classes[label]}\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c06e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e682076",
   "metadata": {},
   "source": [
    "# Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977a07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41369, 17730)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# 70% training, 30% validation\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f7a1d",
   "metadata": {},
   "source": [
    "# Define Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605a718",
   "metadata": {},
   "source": [
    "### Transform Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73360cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresize(...) : CNN models typically require fixed-size inputs, so resizing ensures all images conform to the expected input dimensions.\\n\\nToTensor(...) : Converts images to float32, scales pixel values to [0, 1], and rearranges dimensions to (C, H, W).\\n\\nNormalize(...) : Aligns image data with the distribution of the dataset used to pre-train the CNN models, which helps in faster convergence and better performance.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "resize(...) : CNN models typically require fixed-size inputs, so resizing ensures all images conform to the expected input dimensions.\n",
    "\n",
    "ToTensor(...) : Converts images to float32, scales pixel values to [0, 1], and rearranges dimensions to (C, H, W).\n",
    "\n",
    "Normalize(...) : Aligns image data with the distribution of the dataset used to pre-train the CNN models, which helps in faster convergence and better performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b3e42",
   "metadata": {},
   "source": [
    "### Apply transform to train and val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76ace54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.transform = transform\n",
    "val_dataset.dataset.transform = transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3970a",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee1e0298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), tensor(-1.3130), tensor(2.6400))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = train_dataset[0]\n",
    "img.shape, img.min(), img.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ce585",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c690e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64 # Number of samples per batch\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Prevent learning order bias\n",
    "    num_workers=4,  # Parallel data loading\n",
    "    pin_memory=True  # Speed up data transfer to GPU, harmless if CUDA is on\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc785a",
   "metadata": {},
   "source": [
    "Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de25da84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]), torch.Size([64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc710d1",
   "metadata": {},
   "source": [
    "# Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a384c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\Anaconda\\envs\\ml-gpu\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Coding\\Anaconda\\envs\\ml-gpu\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# Using the model \"MobileNetV2\" pretrained on ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0961ab1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60df5db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9831ac2",
   "metadata": {},
   "source": [
    "in_features=1280 means : model give 1280 number of features the model has extracted from image before making a decision.\n",
    "at the end, model will convert 224x224x3 image into one vector of length 1280\n",
    "\n",
    "Here, out_feature = 1000 is 1000 because ImageNet has 1000 classes including dogs, cats, planes, bananas, etc.\n",
    "\n",
    "We have only 5 classes that are our 5 emotions, so we'll change 1000 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6f0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "NUM_CLASSES = 5  # Angry, Disgust, Fear, Happy, Sad\n",
    "\n",
    "# Replace only the classifier with a new one for our specific task\n",
    "model.classifier[1] = nn.Linear(\n",
    "    in_features=1280,\n",
    "    out_features=NUM_CLASSES,\n",
    "    bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9285f25",
   "metadata": {},
   "source": [
    "nn = torch.nn : this is pytorch's module system for neural networks. Anything having parameters, participating in backprop, and learns lives in torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e24ae0",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0201d462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=1280, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef122f7",
   "metadata": {},
   "source": [
    "Okay, now we have successfully updated our output to 5.\n",
    "\n",
    "Now we will train the model, but before that, we will need the features part of model to remain as it is as it recognises edges and all that will help in recognising the mood. So we will only have to train the classifier head, and make the feature's part remain as it is, so we will first freeze the features weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48802a",
   "metadata": {},
   "source": [
    "## Freezing the model's features part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80167fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():  # All weights in feature extractor\n",
    "    param.requires_grad = False  # Pytorch stops computing gradients for these parameters, optimizer ignores them, weights stay unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4f44e",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f63f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight False\n",
      "features.0.1.weight False\n",
      "features.0.1.bias False\n",
      "features.1.conv.0.0.weight False\n",
      "features.1.conv.0.1.weight False\n",
      "features.1.conv.0.1.bias False\n",
      "features.1.conv.1.weight False\n",
      "features.1.conv.2.weight False\n",
      "features.1.conv.2.bias False\n",
      "features.2.conv.0.0.weight False\n",
      "features.2.conv.0.1.weight False\n",
      "features.2.conv.0.1.bias False\n",
      "features.2.conv.1.0.weight False\n",
      "features.2.conv.1.1.weight False\n",
      "features.2.conv.1.1.bias False\n",
      "features.2.conv.2.weight False\n",
      "features.2.conv.3.weight False\n",
      "features.2.conv.3.bias False\n",
      "features.3.conv.0.0.weight False\n",
      "features.3.conv.0.1.weight False\n",
      "features.3.conv.0.1.bias False\n",
      "features.3.conv.1.0.weight False\n",
      "features.3.conv.1.1.weight False\n",
      "features.3.conv.1.1.bias False\n",
      "features.3.conv.2.weight False\n",
      "features.3.conv.3.weight False\n",
      "features.3.conv.3.bias False\n",
      "features.4.conv.0.0.weight False\n",
      "features.4.conv.0.1.weight False\n",
      "features.4.conv.0.1.bias False\n",
      "features.4.conv.1.0.weight False\n",
      "features.4.conv.1.1.weight False\n",
      "features.4.conv.1.1.bias False\n",
      "features.4.conv.2.weight False\n",
      "features.4.conv.3.weight False\n",
      "features.4.conv.3.bias False\n",
      "features.5.conv.0.0.weight False\n",
      "features.5.conv.0.1.weight False\n",
      "features.5.conv.0.1.bias False\n",
      "features.5.conv.1.0.weight False\n",
      "features.5.conv.1.1.weight False\n",
      "features.5.conv.1.1.bias False\n",
      "features.5.conv.2.weight False\n",
      "features.5.conv.3.weight False\n",
      "features.5.conv.3.bias False\n",
      "features.6.conv.0.0.weight False\n",
      "features.6.conv.0.1.weight False\n",
      "features.6.conv.0.1.bias False\n",
      "features.6.conv.1.0.weight False\n",
      "features.6.conv.1.1.weight False\n",
      "features.6.conv.1.1.bias False\n",
      "features.6.conv.2.weight False\n",
      "features.6.conv.3.weight False\n",
      "features.6.conv.3.bias False\n",
      "features.7.conv.0.0.weight False\n",
      "features.7.conv.0.1.weight False\n",
      "features.7.conv.0.1.bias False\n",
      "features.7.conv.1.0.weight False\n",
      "features.7.conv.1.1.weight False\n",
      "features.7.conv.1.1.bias False\n",
      "features.7.conv.2.weight False\n",
      "features.7.conv.3.weight False\n",
      "features.7.conv.3.bias False\n",
      "features.8.conv.0.0.weight False\n",
      "features.8.conv.0.1.weight False\n",
      "features.8.conv.0.1.bias False\n",
      "features.8.conv.1.0.weight False\n",
      "features.8.conv.1.1.weight False\n",
      "features.8.conv.1.1.bias False\n",
      "features.8.conv.2.weight False\n",
      "features.8.conv.3.weight False\n",
      "features.8.conv.3.bias False\n",
      "features.9.conv.0.0.weight False\n",
      "features.9.conv.0.1.weight False\n",
      "features.9.conv.0.1.bias False\n",
      "features.9.conv.1.0.weight False\n",
      "features.9.conv.1.1.weight False\n",
      "features.9.conv.1.1.bias False\n",
      "features.9.conv.2.weight False\n",
      "features.9.conv.3.weight False\n",
      "features.9.conv.3.bias False\n",
      "features.10.conv.0.0.weight False\n",
      "features.10.conv.0.1.weight False\n",
      "features.10.conv.0.1.bias False\n",
      "features.10.conv.1.0.weight False\n",
      "features.10.conv.1.1.weight False\n",
      "features.10.conv.1.1.bias False\n",
      "features.10.conv.2.weight False\n",
      "features.10.conv.3.weight False\n",
      "features.10.conv.3.bias False\n",
      "features.11.conv.0.0.weight False\n",
      "features.11.conv.0.1.weight False\n",
      "features.11.conv.0.1.bias False\n",
      "features.11.conv.1.0.weight False\n",
      "features.11.conv.1.1.weight False\n",
      "features.11.conv.1.1.bias False\n",
      "features.11.conv.2.weight False\n",
      "features.11.conv.3.weight False\n",
      "features.11.conv.3.bias False\n",
      "features.12.conv.0.0.weight False\n",
      "features.12.conv.0.1.weight False\n",
      "features.12.conv.0.1.bias False\n",
      "features.12.conv.1.0.weight False\n",
      "features.12.conv.1.1.weight False\n",
      "features.12.conv.1.1.bias False\n",
      "features.12.conv.2.weight False\n",
      "features.12.conv.3.weight False\n",
      "features.12.conv.3.bias False\n",
      "features.13.conv.0.0.weight False\n",
      "features.13.conv.0.1.weight False\n",
      "features.13.conv.0.1.bias False\n",
      "features.13.conv.1.0.weight False\n",
      "features.13.conv.1.1.weight False\n",
      "features.13.conv.1.1.bias False\n",
      "features.13.conv.2.weight False\n",
      "features.13.conv.3.weight False\n",
      "features.13.conv.3.bias False\n",
      "features.14.conv.0.0.weight False\n",
      "features.14.conv.0.1.weight False\n",
      "features.14.conv.0.1.bias False\n",
      "features.14.conv.1.0.weight False\n",
      "features.14.conv.1.1.weight False\n",
      "features.14.conv.1.1.bias False\n",
      "features.14.conv.2.weight False\n",
      "features.14.conv.3.weight False\n",
      "features.14.conv.3.bias False\n",
      "features.15.conv.0.0.weight False\n",
      "features.15.conv.0.1.weight False\n",
      "features.15.conv.0.1.bias False\n",
      "features.15.conv.1.0.weight False\n",
      "features.15.conv.1.1.weight False\n",
      "features.15.conv.1.1.bias False\n",
      "features.15.conv.2.weight False\n",
      "features.15.conv.3.weight False\n",
      "features.15.conv.3.bias False\n",
      "features.16.conv.0.0.weight False\n",
      "features.16.conv.0.1.weight False\n",
      "features.16.conv.0.1.bias False\n",
      "features.16.conv.1.0.weight False\n",
      "features.16.conv.1.1.weight False\n",
      "features.16.conv.1.1.bias False\n",
      "features.16.conv.2.weight False\n",
      "features.16.conv.3.weight False\n",
      "features.16.conv.3.bias False\n",
      "features.17.conv.0.0.weight False\n",
      "features.17.conv.0.1.weight False\n",
      "features.17.conv.0.1.bias False\n",
      "features.17.conv.1.0.weight False\n",
      "features.17.conv.1.1.weight False\n",
      "features.17.conv.1.1.bias False\n",
      "features.17.conv.2.weight False\n",
      "features.17.conv.3.weight False\n",
      "features.17.conv.3.bias False\n",
      "features.18.0.weight False\n",
      "features.18.1.weight False\n",
      "features.18.1.bias False\n",
      "classifier.1.weight True\n",
      "classifier.1.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e1b5d",
   "metadata": {},
   "source": [
    "## Move the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84ca27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device  # device is just a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03fba3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)  # .to(device) moves physically moves tensors + parameters to the specified device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ad914",
   "metadata": {},
   "source": [
    "feature extractor stays frozen but still moves to gpu inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22203c1",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95583eb9",
   "metadata": {},
   "source": [
    "what loss does : Loss answers one question \"How wrong was the model ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c0732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# CrossEntropyLoss combines nn.LogSoftmax() and nn.NLLLoss() in one single class. i.e., it applies Softmax to the model outputs, compares them with the labels, and computes the loss and produces one scalar number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e93f90",
   "metadata": {},
   "source": [
    "## Define optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d24bad",
   "metadata": {},
   "source": [
    "Just to ensure we do not pass model.parameters() because that will waste memory and lose the clarity, we will explicitly pass classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abd81a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.classifier.parameters(),  # Only optimize the classifier parameters\n",
    "    lr=1e-3 # Learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e684f2c",
   "metadata": {},
   "source": [
    "One forward pass sanity check.\n",
    "\n",
    "Before training loops , we check data > model > output, if the shapes have lined up, and if gpu usage is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab7e38",
   "metadata": {},
   "source": [
    "## Forward pass check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ee5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(train_loader))\n",
    "\n",
    "# images = images.to(device)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "# outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6706651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5380e-01,  1.1352e-03, -6.8130e-02,  1.1903e-01, -1.4448e-01],\n",
       "        [ 7.1477e-02, -7.4851e-01,  2.7621e-02, -6.3523e-02, -6.8241e-01],\n",
       "        [ 3.9731e-01,  3.1461e-02, -1.6884e-01,  5.9638e-02,  2.7402e-01],\n",
       "        [ 1.8810e-01,  2.5361e-01,  2.3815e-01, -1.4736e-01, -1.9315e-01],\n",
       "        [ 6.1781e-01,  4.5060e-01,  3.6684e-01, -6.6034e-01, -6.3672e-01],\n",
       "        [ 7.4274e-01,  2.6970e-01, -3.4338e-01,  7.2573e-02, -1.9509e-01],\n",
       "        [-4.1241e-02, -3.9403e-01, -6.2505e-01, -1.9556e-01, -2.8439e-01],\n",
       "        [ 7.0859e-02, -3.7671e-01, -3.1892e-01,  3.7150e-01, -5.1658e-01],\n",
       "        [ 1.4404e-01, -2.3258e-01, -1.9774e-01,  2.0736e-01,  3.2720e-03],\n",
       "        [ 2.3284e-01,  3.1748e-01,  2.6926e-02, -1.9206e-01, -6.2747e-01],\n",
       "        [ 7.2198e-02, -3.9238e-01,  4.4107e-01,  7.0055e-02, -2.7715e-01],\n",
       "        [ 3.3724e-01, -2.1313e-01, -1.4312e-01,  2.6959e-01, -5.3875e-01],\n",
       "        [ 8.2022e-01,  3.6587e-01, -2.0268e-01,  1.9506e-01, -4.4981e-01],\n",
       "        [ 7.4076e-01,  2.5676e-02, -1.0249e-01, -3.8485e-02,  6.8050e-02],\n",
       "        [ 9.1697e-02, -7.1865e-01, -1.9829e-01, -6.4449e-01,  2.2257e-01],\n",
       "        [ 4.1758e-02, -7.1666e-02,  1.7500e-01, -7.4151e-01, -5.5512e-02],\n",
       "        [ 5.8364e-01,  7.0411e-01,  8.1662e-01,  3.7443e-01, -5.7041e-01],\n",
       "        [ 4.7927e-01, -3.2917e-01,  1.8849e-01,  3.1266e-02, -6.9011e-02],\n",
       "        [-1.5614e-02,  9.4661e-02, -2.9905e-01, -1.0526e-01,  3.1261e-01],\n",
       "        [ 4.8076e-01,  2.3595e-01, -4.2389e-01, -1.1776e-01, -3.5497e-01],\n",
       "        [ 1.3622e-01,  3.4643e-02,  2.7044e-01,  3.4444e-01, -5.5169e-01],\n",
       "        [ 4.8125e-01, -3.4467e-02, -1.4236e-01, -1.8652e-01,  3.7496e-02],\n",
       "        [ 1.1951e+00,  3.3043e-03,  1.0891e-01, -8.3195e-02,  3.6387e-02],\n",
       "        [ 1.8993e-01,  6.3289e-02, -1.6313e-01,  3.0527e-02, -6.6512e-01],\n",
       "        [ 6.3036e-01, -1.5509e-01, -1.8496e-02, -1.0806e-01, -8.1778e-02],\n",
       "        [ 8.4110e-01, -4.1280e-01, -3.7863e-02,  3.1802e-01, -1.1723e-01],\n",
       "        [ 3.7610e-01, -4.7416e-01,  2.6607e-01, -7.0332e-01,  5.7674e-01],\n",
       "        [ 6.9780e-01,  1.7181e-01, -3.7633e-01, -1.4907e-03,  4.8347e-01],\n",
       "        [-1.3999e-02, -4.0980e-01, -1.0284e-01,  7.6006e-03, -1.0956e+00],\n",
       "        [ 4.3492e-01, -2.4959e-02,  1.7640e-01,  9.7230e-02,  6.1997e-02],\n",
       "        [ 5.4375e-01, -2.5084e-01, -4.2800e-01,  1.5715e-01, -9.9049e-02],\n",
       "        [ 2.8260e-01,  2.8524e-01, -4.8913e-02,  1.9681e-01, -1.3508e-01],\n",
       "        [ 6.1693e-01, -1.1937e-01, -3.6190e-01,  2.7215e-01,  2.0324e-02],\n",
       "        [ 3.4727e-01, -2.4778e-01, -2.4524e-01,  5.6928e-01, -3.9746e-01],\n",
       "        [ 4.2416e-01,  3.4132e-01, -1.2658e-01, -2.3292e-01, -1.9987e-01],\n",
       "        [ 1.4385e-01, -3.1875e-02, -2.7905e-01, -3.5667e-01, -3.5468e-01],\n",
       "        [ 2.6877e-01, -3.6759e-01,  4.5555e-01, -1.8514e-01, -3.5827e-01],\n",
       "        [ 2.3407e-01,  2.1711e-01,  4.0796e-01, -1.3511e-01, -2.5204e-01],\n",
       "        [ 6.6305e-01,  1.2448e-01,  6.3420e-01,  2.4415e-01, -4.3136e-01],\n",
       "        [ 7.4980e-01,  3.2495e-01,  1.4122e-01, -1.9814e-01, -5.2071e-02],\n",
       "        [ 8.2309e-01, -2.1262e-01,  6.7663e-02,  4.5371e-01,  7.3882e-02],\n",
       "        [ 9.9319e-02, -7.0240e-02, -1.1820e-01, -3.2892e-01, -6.4443e-02],\n",
       "        [ 2.6371e-01,  1.7619e-01, -2.2041e-01,  1.6092e-01,  7.7682e-02],\n",
       "        [ 8.4272e-01,  1.8068e-01,  6.6916e-01, -2.0761e-01, -2.8164e-01],\n",
       "        [ 6.0970e-01, -1.5304e-01, -3.4341e-01, -8.1251e-02, -2.3115e-01],\n",
       "        [ 4.1497e-02,  1.4036e-01, -3.8434e-02,  1.7110e-01, -2.3251e-01],\n",
       "        [ 6.0571e-01, -2.7143e-01, -2.8868e-01, -1.8990e-01, -2.1663e-01],\n",
       "        [ 5.1962e-01, -2.4612e-01,  3.3802e-02,  1.2821e-01, -1.3284e-01],\n",
       "        [ 8.0147e-01, -5.7937e-01, -6.8667e-01, -7.0738e-01,  1.9840e-02],\n",
       "        [ 4.0838e-01, -1.8097e-01, -6.0920e-02,  2.5544e-01, -4.7553e-01],\n",
       "        [ 5.4448e-01, -8.7254e-02,  5.6981e-02,  2.3647e-01, -3.2904e-01],\n",
       "        [ 8.2811e-01, -3.1092e-01, -2.5766e-01,  1.2907e-01,  6.5385e-01],\n",
       "        [ 4.8055e-01,  1.7792e-01,  3.6520e-01,  1.5286e-01,  5.1975e-01],\n",
       "        [-5.8276e-01, -2.0401e-01, -5.4943e-01, -3.1941e-01,  2.8656e-02],\n",
       "        [ 3.8762e-01,  3.4706e-01, -5.6010e-01, -2.8213e-01, -9.6706e-02],\n",
       "        [ 7.9323e-01, -9.4632e-01,  1.9321e-01,  6.9842e-02, -2.9043e-01],\n",
       "        [ 5.4737e-01, -3.7092e-01, -3.1572e-01, -1.7938e-01, -1.4237e-01],\n",
       "        [ 4.0945e-01,  1.4520e-01, -4.3889e-02, -5.8367e-01, -1.9913e-01],\n",
       "        [ 4.1348e-01, -1.1157e-01, -4.4853e-01, -6.2309e-02, -4.9049e-02],\n",
       "        [ 2.1159e-01,  3.2564e-01,  3.4433e-01, -3.7978e-01,  1.8672e-02],\n",
       "        [-1.2373e-01, -4.5751e-01,  9.8760e-02, -1.7056e-01, -2.4066e-01],\n",
       "        [ 3.5508e-01, -7.3905e-02,  6.3902e-02,  9.4656e-02, -4.4010e-01],\n",
       "        [ 4.3609e-01,  2.2857e-02,  1.9152e-01, -9.1236e-02,  2.8908e-02],\n",
       "        [ 6.5309e-01,  8.4079e-02,  1.4432e-01, -4.9672e-01, -2.3570e-01]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd040c",
   "metadata": {},
   "source": [
    "# Training loops\n",
    "\n",
    "what a training loop is : guess > check how wrong > adjust > repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132110b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.3233\n",
      "Epoch [2/3], Loss: 1.2593\n",
      "Epoch [3/3], Loss: 1.2483\n"
     ]
    }
   ],
   "source": [
    "# EPOCHS = 3\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "    \n",
    "#     model.train()  # Set model to training mode\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "#         outputs = model(images)  # Forward pass\n",
    "#         loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update weights\n",
    "\n",
    "#         running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "#     avg_loss = running_loss / len(train_loader)\n",
    "#     print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee1d02",
   "metadata": {},
   "source": [
    "The given decrease in loss means that the model is learning but not able to attain more accuracy.\n",
    "\n",
    "The fix is NOT : more epochs, bigger batch, or more patience,\n",
    "\n",
    "The fix IS : fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd3ca6",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3ad8a",
   "metadata": {},
   "source": [
    "### Step 1 : Unfreeze last feature block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "101a4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d332c",
   "metadata": {},
   "source": [
    "### Step 2 : Update optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d442397",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918cf00",
   "metadata": {},
   "source": [
    "Lower LR = stable fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07d75bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight True\n",
      "features.0.1.weight True\n",
      "features.0.1.bias True\n",
      "features.1.conv.0.0.weight True\n",
      "features.1.conv.0.1.weight True\n",
      "features.1.conv.0.1.bias True\n",
      "features.1.conv.1.weight True\n",
      "features.1.conv.2.weight True\n",
      "features.1.conv.2.bias True\n",
      "features.2.conv.0.0.weight True\n",
      "features.2.conv.0.1.weight True\n",
      "features.2.conv.0.1.bias True\n",
      "features.2.conv.1.0.weight True\n",
      "features.2.conv.1.1.weight True\n",
      "features.2.conv.1.1.bias True\n",
      "features.2.conv.2.weight True\n",
      "features.2.conv.3.weight True\n",
      "features.2.conv.3.bias True\n",
      "features.3.conv.0.0.weight True\n",
      "features.3.conv.0.1.weight True\n",
      "features.3.conv.0.1.bias True\n",
      "features.3.conv.1.0.weight True\n",
      "features.3.conv.1.1.weight True\n",
      "features.3.conv.1.1.bias True\n",
      "features.3.conv.2.weight True\n",
      "features.3.conv.3.weight True\n",
      "features.3.conv.3.bias True\n",
      "features.4.conv.0.0.weight True\n",
      "features.4.conv.0.1.weight True\n",
      "features.4.conv.0.1.bias True\n",
      "features.4.conv.1.0.weight True\n",
      "features.4.conv.1.1.weight True\n",
      "features.4.conv.1.1.bias True\n",
      "features.4.conv.2.weight True\n",
      "features.4.conv.3.weight True\n",
      "features.4.conv.3.bias True\n",
      "features.5.conv.0.0.weight True\n",
      "features.5.conv.0.1.weight True\n",
      "features.5.conv.0.1.bias True\n",
      "features.5.conv.1.0.weight True\n",
      "features.5.conv.1.1.weight True\n",
      "features.5.conv.1.1.bias True\n",
      "features.5.conv.2.weight True\n",
      "features.5.conv.3.weight True\n",
      "features.5.conv.3.bias True\n",
      "features.6.conv.0.0.weight True\n",
      "features.6.conv.0.1.weight True\n",
      "features.6.conv.0.1.bias True\n",
      "features.6.conv.1.0.weight True\n",
      "features.6.conv.1.1.weight True\n",
      "features.6.conv.1.1.bias True\n",
      "features.6.conv.2.weight True\n",
      "features.6.conv.3.weight True\n",
      "features.6.conv.3.bias True\n",
      "features.7.conv.0.0.weight True\n",
      "features.7.conv.0.1.weight True\n",
      "features.7.conv.0.1.bias True\n",
      "features.7.conv.1.0.weight True\n",
      "features.7.conv.1.1.weight True\n",
      "features.7.conv.1.1.bias True\n",
      "features.7.conv.2.weight True\n",
      "features.7.conv.3.weight True\n",
      "features.7.conv.3.bias True\n",
      "features.8.conv.0.0.weight True\n",
      "features.8.conv.0.1.weight True\n",
      "features.8.conv.0.1.bias True\n",
      "features.8.conv.1.0.weight True\n",
      "features.8.conv.1.1.weight True\n",
      "features.8.conv.1.1.bias True\n",
      "features.8.conv.2.weight True\n",
      "features.8.conv.3.weight True\n",
      "features.8.conv.3.bias True\n",
      "features.9.conv.0.0.weight True\n",
      "features.9.conv.0.1.weight True\n",
      "features.9.conv.0.1.bias True\n",
      "features.9.conv.1.0.weight True\n",
      "features.9.conv.1.1.weight True\n",
      "features.9.conv.1.1.bias True\n",
      "features.9.conv.2.weight True\n",
      "features.9.conv.3.weight True\n",
      "features.9.conv.3.bias True\n",
      "features.10.conv.0.0.weight True\n",
      "features.10.conv.0.1.weight True\n",
      "features.10.conv.0.1.bias True\n",
      "features.10.conv.1.0.weight True\n",
      "features.10.conv.1.1.weight True\n",
      "features.10.conv.1.1.bias True\n",
      "features.10.conv.2.weight True\n",
      "features.10.conv.3.weight True\n",
      "features.10.conv.3.bias True\n",
      "features.11.conv.0.0.weight True\n",
      "features.11.conv.0.1.weight True\n",
      "features.11.conv.0.1.bias True\n",
      "features.11.conv.1.0.weight True\n",
      "features.11.conv.1.1.weight True\n",
      "features.11.conv.1.1.bias True\n",
      "features.11.conv.2.weight True\n",
      "features.11.conv.3.weight True\n",
      "features.11.conv.3.bias True\n",
      "features.12.conv.0.0.weight True\n",
      "features.12.conv.0.1.weight True\n",
      "features.12.conv.0.1.bias True\n",
      "features.12.conv.1.0.weight True\n",
      "features.12.conv.1.1.weight True\n",
      "features.12.conv.1.1.bias True\n",
      "features.12.conv.2.weight True\n",
      "features.12.conv.3.weight True\n",
      "features.12.conv.3.bias True\n",
      "features.13.conv.0.0.weight True\n",
      "features.13.conv.0.1.weight True\n",
      "features.13.conv.0.1.bias True\n",
      "features.13.conv.1.0.weight True\n",
      "features.13.conv.1.1.weight True\n",
      "features.13.conv.1.1.bias True\n",
      "features.13.conv.2.weight True\n",
      "features.13.conv.3.weight True\n",
      "features.13.conv.3.bias True\n",
      "features.14.conv.0.0.weight True\n",
      "features.14.conv.0.1.weight True\n",
      "features.14.conv.0.1.bias True\n",
      "features.14.conv.1.0.weight True\n",
      "features.14.conv.1.1.weight True\n",
      "features.14.conv.1.1.bias True\n",
      "features.14.conv.2.weight True\n",
      "features.14.conv.3.weight True\n",
      "features.14.conv.3.bias True\n",
      "features.15.conv.0.0.weight True\n",
      "features.15.conv.0.1.weight True\n",
      "features.15.conv.0.1.bias True\n",
      "features.15.conv.1.0.weight True\n",
      "features.15.conv.1.1.weight True\n",
      "features.15.conv.1.1.bias True\n",
      "features.15.conv.2.weight True\n",
      "features.15.conv.3.weight True\n",
      "features.15.conv.3.bias True\n",
      "features.16.conv.0.0.weight True\n",
      "features.16.conv.0.1.weight True\n",
      "features.16.conv.0.1.bias True\n",
      "features.16.conv.1.0.weight True\n",
      "features.16.conv.1.1.weight True\n",
      "features.16.conv.1.1.bias True\n",
      "features.16.conv.2.weight True\n",
      "features.16.conv.3.weight True\n",
      "features.16.conv.3.bias True\n",
      "features.17.conv.0.0.weight True\n",
      "features.17.conv.0.1.weight True\n",
      "features.17.conv.0.1.bias True\n",
      "features.17.conv.1.0.weight True\n",
      "features.17.conv.1.1.weight True\n",
      "features.17.conv.1.1.bias True\n",
      "features.17.conv.2.weight True\n",
      "features.17.conv.3.weight True\n",
      "features.17.conv.3.bias True\n",
      "features.18.0.weight True\n",
      "features.18.1.weight True\n",
      "features.18.1.bias True\n",
      "classifier.1.weight True\n",
      "classifier.1.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1df9bf",
   "metadata": {},
   "source": [
    "### Step 3 : Training Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.8620\n",
      "Epoch [2/3], Loss: 0.5777\n",
      "Epoch [3/3], Loss: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# EPOCHS = 3\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "    \n",
    "#     model.train()  # Set model to training mode\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "#         outputs = model(images)  # Forward pass\n",
    "#         loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update weights\n",
    "\n",
    "#         running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # avg_loss = running_loss / len(train_loader)\n",
    "    # print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b98ca",
   "metadata": {},
   "source": [
    "# Validation process (Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a30319",
   "metadata": {},
   "source": [
    "How validation is computed :\n",
    "1. take a batch of validation images\n",
    "2. run them through model\n",
    "3. get outputs (logits)\n",
    "4. Pick the highest score - predicted class\n",
    "5. Compare with true label\n",
    "6. Count correct predictions\n",
    "7. Divide by total\n",
    "\n",
    "No gradients\n",
    "\n",
    "no weigh updates\n",
    "\n",
    "no learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f454123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation, no learning during validation\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41852a2c",
   "metadata": {},
   "source": [
    "Now we'll write a code to validate accuracy of model after every epoch so that we'll understand if the model is increasing in accuracy or not, is it overfitting, should we stop early ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdf36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    val_acc = validate(model, val_loader, device)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}]\\n\", f\"Loss: {avg_loss:.4f}\\n\" f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
